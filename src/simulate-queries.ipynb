{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/higherbar-ai/open-chat-studio-sim/blob/main/src/simulate-queries.ipynb\" target=\"_parent\"><img alt=\"Open In Colab\" src=\"https://colab.research.google.com/assets/colab-badge.svg\"/></a>\n",
    "\n",
    "# Execute fixed queries\n",
    "\n",
    "This notebook executes a fixed set of single-turn queries against an Open Chat Studio experiment.\n",
    "\n",
    "## Running in Google Colab\n",
    "\n",
    "Before running this notebook, you'll need to configure a series of secrets in Google Colab; click the key button in the left sidebar, and be sure to click the toggle to give this notebook access to each of the secrets. These are the secrets used by this notebook:\n",
    "\n",
    "- `OCS_API_KEY`: your Open Chat Studio API key\n",
    "- `ATHINA_API_KEY`: your Athina API key (optional; only if you want to export results to Athina)\n",
    "- `EXPERIMENT_ID`: the ID of the experiment you want to issue queries to\n",
    "- `PARTICIPANT_ID`: the participant ID to use for the queries\n",
    "\n",
    "## Running in a local environment\n",
    "\n",
    "When you first run the first code cell in this notebook, it will output a template configuration file for you. Edit that file to specify your configuration parameters (see above for their descriptions). \n",
    "\n",
    "## Selecting or uploading your queries to run\n",
    "\n",
    "The second code cell will prompt you to select or upload a .csv file with the queries you want to run. This file should have the following columns:\n",
    "\n",
    "- `query`: the query to send to the experiment\n",
    "- `query_id`: (optional) a unique identifier for the query (if not provided, row number will be used)\n",
    "- `expected_response`: (optional) the expected response to the query\n",
    "\n",
    "## Where results go\n",
    "\n",
    "The results of the queries will be saved to a file called `query_results.csv`. If you're running in Google Colab, click the folder button in the sidebar to view and download that file. If you're running locally, it will be output to the `ocs` subdirectory off of your local directory. \n",
    "\n",
    "If an Athina API key is configured, the results will also be exported to an Athina dataset."
   ],
   "id": "b81674964944ba23"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First, we need to set up our environment handler\n",
    "try:\n",
    "    # Try importing directly (local environment)\n",
    "    from colab_or_local_env import ColabOrLocalEnv  # type: ignore[import]\n",
    "except ImportError:\n",
    "    # If import fails, we're probably in Colab, so fetch the file\n",
    "    import requests\n",
    "    \n",
    "    # Fetch the environment handler\n",
    "    url = \"https://raw.githubusercontent.com/higherbar-ai/open-chat-studio-sim/main/src/colab_or_local_env.py\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Save it to the current directory\n",
    "    with open(\"colab_or_local_env.py\", \"w\") as f:\n",
    "        f.write(response.text)\n",
    "    \n",
    "    # Now we can import it\n",
    "    from colab_or_local_env import ColabOrLocalEnv\n",
    "    \n",
    "# set log level to WARNING\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "\n",
    "# Initialize our environment\n",
    "env = ColabOrLocalEnv(\n",
    "    github_repo=\"higherbar-ai/open-chat-studio-sim\",\n",
    "    requirements_path=\"requirements.txt\",\n",
    "    module_paths=[\"src/ocs_api.py\", \"src/ocs_simulation_support.py\"],\n",
    "    config_path=\"~/.ocs/.env\",\n",
    "    config_template={\n",
    "        \"OCS_API_KEY\": \"\",\n",
    "        \"ATHINA_API_KEY\": \"\",\n",
    "        \"EXPERIMENT_ID\": \"\",\n",
    "        \"PARTICIPANT_ID\": \"open-chat-studio-sim\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# Set up the environment (install requirements, fetch modules if needed)\n",
    "env.setup_environment()\n",
    "\n",
    "# Internal configuration\n",
    "api_timeout_seconds = 300      # how long to give API calls before timing out\n",
    "api_num_retries = 3            # how many times to retry API calls before giving up\n",
    "api_retry_delay_seconds = 2    # how long to wait between retries\n",
    "continue_on_error = True       # whether to record errors and continue (if False, errors will halt execution)\n",
    "\n",
    "# Get API keys from environment\n",
    "ocs_api_key = env.get_config_setting(\"OCS_API_KEY\")\n",
    "athina_api_key = env.get_config_setting(\"ATHINA_API_KEY\")\n",
    "participant_id = env.get_config_setting(\"PARTICIPANT_ID\")\n",
    "experiment_id = env.get_config_setting(\"EXPERIMENT_ID\")\n",
    "\n",
    "# Validate required configuration\n",
    "if not all([ocs_api_key, participant_id, experiment_id]):\n",
    "    raise ValueError(\"Please supply at least OCS_API_KEY, PARTICIPANT_ID, and EXPERIMENT_ID in your secrets or configuration file.\")\n",
    "\n",
    "# Output files to ~/ocs directory if local, otherwise /content if Google Colab\n",
    "if env.is_colab:\n",
    "    output_path_prefix = \"/content\"\n",
    "else:\n",
    "    import os\n",
    "    output_path_prefix = os.path.expanduser(\"~/ocs\")\n",
    "    os.makedirs(output_path_prefix, exist_ok=True)\n",
    "\n",
    "# Initialize OCS API support\n",
    "from ocs_api import OCSAPIClient    # type: ignore[import]\n",
    "ocs_api_client = OCSAPIClient(\n",
    "    api_key=ocs_api_key, \n",
    "    timeout_seconds=api_timeout_seconds, \n",
    "    num_retries=api_num_retries, \n",
    "    retry_wait_seconds=api_retry_delay_seconds\n",
    ")\n",
    "\n",
    "# Report results\n",
    "print(f\"Configuration loaded for {'Colab' if env.is_colab else 'local'} environment, OCS API initialized.\")"
   ],
   "id": "18e0da6d5b404a63",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Select or upload your queries to run\n",
    "\n",
    "The code cell below will prompt you to select or upload a .csv file. That file should have the following columns:\n",
    "\n",
    "- `query`: the query to send to the experiment\n",
    "- `query_id`: (optional) a unique identifier for the query (if not provided, row number will be used)\n",
    "- `expected_response`: (optional) the expected response to the query"
   ],
   "id": "53612609dcb64202"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# prompt for the CSV file with queries to run\n",
    "queries_to_run_files = env.get_input_files(\"CSV file with queries to run\")\n",
    "\n",
    "# check for one CSV file\n",
    "if len(queries_to_run_files) != 1:\n",
    "    raise ValueError(\"Please select exactly one CSV file with queries to run.\")\n",
    "elif not queries_to_run_files[0].endswith(\".csv\"):\n",
    "    raise ValueError(\"Please select a CSV file with queries to run.\")\n",
    "\n",
    "queries_to_run_file = str(queries_to_run_files[0])"
   ],
   "id": "81e0452e20826e21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Execute queries\n",
    "\n",
    "The following code block reads a list of simulations to run from the .csv file selected or uploaded above, executes them, and saves the results to the `query_results.csv` file. \n",
    "\n",
    "Your .csv file should have the following columns:\n",
    "\n",
    "- `query`: the query to send to the experiment\n",
    "- `query_id`: (optional) a unique identifier for the query (if not provided, row number will be used)\n",
    "- `expected_response`: (optional) the expected response to the query\n",
    "\n",
    "`query_results.csv` will have the following columns:\n",
    "\n",
    "- `query_id`: the unique identifier for the query\n",
    "- `session_id`: the unique identifier for the session\n",
    "- `query`: the query sent to the AI assistant\n",
    "- `response`: the response received from the AI assistant\n",
    "- `expected_response`: (optional) the expected response for the query (if provided in the input file)"
   ],
   "id": "c71c645a2078bc1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# load input file using pandas\n",
    "queries_to_run = pd.read_csv(queries_to_run_file)\n",
    "\n",
    "# run through and execute each of the queries\n",
    "results = []\n",
    "for index, row in queries_to_run.iterrows():\n",
    "    # initialize query details\n",
    "    query_id = str(row.get(\"query_id\", index+1))\n",
    "    query = row[\"query\"]\n",
    "    expected_response = row.get(\"expected_response\", \"\")\n",
    "    response = \"\"\n",
    "    session_id = \"\"\n",
    "    \n",
    "    # report out\n",
    "    print(f\"Executing query {query_id}...\")\n",
    "    \n",
    "    # execute query, catching and logging any errors\n",
    "    try:\n",
    "        # create a new session for the query\n",
    "        api_response = ocs_api_client.create_experiment_session(experiment_id, participant_id)\n",
    "        session_id = api_response[\"id\"]\n",
    "    \n",
    "        # send the query to the experiment\n",
    "        api_response = ocs_api_client.send_new_api_message(experiment_id, query, session_id)\n",
    "        response = api_response[\"response\"]\n",
    "    except Exception as e:\n",
    "        if continue_on_error:\n",
    "            # log the error and continue to the next query\n",
    "            logging.error(f\"Continuing following query error: {str(e)}\")\n",
    "            response = f\"ERROR: {str(e)}\"\n",
    "        else:\n",
    "            # raise the error to halt execution\n",
    "            raise\n",
    "\n",
    "    # add to results\n",
    "    results.append({\n",
    "        \"query_id\": query_id,\n",
    "        \"session_id\": session_id,\n",
    "        \"query\": query,\n",
    "        \"response\": response\n",
    "    })\n",
    "    # optionally add expected response to results\n",
    "    if \"expected_response\" in queries_to_run.columns:\n",
    "        results[-1][\"expected_response\"] = expected_response\n",
    "\n",
    "# save results to output .csv file\n",
    "output_file = os.path.join(output_path_prefix, \"query_results.csv\")\n",
    "output_rows = []\n",
    "fieldnames=[\"query_id\", \"session_id\", \"query\", \"response\"]\n",
    "# if there's an expected_response column in the input file, include it in the output\n",
    "if \"expected_response\" in queries_to_run.columns:\n",
    "    fieldnames.append(\"expected_response\")\n",
    "with open(output_file, \"w\", newline=\"\") as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames, quoting=csv.QUOTE_NONNUMERIC, escapechar='\\\\')\n",
    "    writer.writeheader()\n",
    "    for result in results:\n",
    "        # output and record for potential next steps\n",
    "        writer.writerow(result)\n",
    "        output_rows.append(result)\n",
    "\n",
    "# report results\n",
    "print()\n",
    "print(f\"Queries executed and {len(results)} results saved to {output_file}.\")"
   ],
   "id": "5667cf21b85d77a6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Optional: Export results to Athina dataset\n",
    "\n",
    "If an Athina API key is configured, the results can be exported to an Athina dataset. The dataset will be named `queries-{experiment_id}-{timestamp}` and will contain the rows from the `query_results.csv` file."
   ],
   "id": "61e4b07bac0c1755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from ocs_simulation_support import athina_create_dataset    # type: ignore[import]\n",
    "\n",
    "# optionally export the results to an Athina dataset\n",
    "if athina_api_key:\n",
    "    # push new dataset to Athina\n",
    "    dataset_name = f\"queries-{experiment_id}-{pd.Timestamp.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "    dataset_description = f\"Simulated queries for experiment {experiment_id} at {pd.Timestamp.now()}\"\n",
    "    try:\n",
    "        dataset = athina_create_dataset(athina_api_key=athina_api_key, dataset_name=dataset_name, dataset_description=dataset_description, dataset_rows=output_rows)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create Athina dataset: {e}\")\n",
    "    else:\n",
    "        print(f\"Results exported to Athina dataset {dataset.id} (name: {dataset_name}).\")"
   ],
   "id": "420b2ccb22a29b94",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
